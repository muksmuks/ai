"use strict";(self.webpackChunkMukesh_K_s_The_Art_of_AI=self.webpackChunkMukesh_K_s_The_Art_of_AI||[]).push([[621],{1173:function(e,n,t){t.d(n,{p:function(){return g},A:function(){return p}});var a=t(6540),i=t(557),r=t(6835),l=t(3328),o=t(7715),s=t(7169);var c=e=>{let{post:n}=e;return null};const m=["16px","8px","4px"].map((e=>`rgba(0, 0, 0, 0.1) 0px ${e} ${e} 0px`));var d=e=>{let{data:{post:n},children:t}=e;return(0,i.Y)(l.A,null,(0,i.Y)(r.DZ,{as:"h1",variant:"styles.h1"},n.title),(0,i.Y)("p",{sx:{color:"secondary",mt:3,a:{color:"secondary"},fontSize:[1,1,2]}},(0,i.Y)("time",null,n.date),n.tags&&(0,i.Y)(a.Fragment,null," — ",(0,i.Y)(o.A,{tags:n.tags})),n.timeToRead&&" — ",n.timeToRead&&(0,i.Y)("span",null,n.timeToRead," min read")),(0,i.Y)("section",{sx:{my:5,".gatsby-resp-image-wrapper":{my:[4,4,5],borderRadius:"4px",boxShadow:m.join(", "),".gatsby-resp-image-image":{borderRadius:"4px"}},variant:"layout.content"}},t),(0,i.Y)(c,{post:n}))};const g=e=>{var n,t,a;let{data:{post:r}}=e;return(0,i.Y)(s.A,{title:r.title,description:r.description?r.description:r.excerpt,image:r.banner?null===(n=r.banner)||void 0===n||null===(t=n.childImageSharp)||void 0===t||null===(a=t.resize)||void 0===a?void 0:a.src:void 0,pathname:r.slug,canonicalUrl:r.canonicalUrl})};function p(e){let{...n}=e;return a.createElement(d,n)}},7715:function(e,n,t){var a=t(557),i=t(6540),r=t(4794),l=t(3601),o=t(2174);n.A=e=>{let{tags:n}=e;const{tagsPath:t,basePath:s}=(0,l.A)();return(0,a.Y)(i.Fragment,null,n.map(((e,n)=>(0,a.Y)(i.Fragment,{key:e.slug},!!n&&", ",(0,a.Y)(r.Link,{sx:e=>{var n;return{...null===(n=e.styles)||void 0===n?void 0:n.a}},to:(0,o.A)(`/${s}/${t}/${e.slug}`)},e.name)))))}},7169:function(e,n,t){var a=t(6540),i=t(4794),r=t(7533);n.A=e=>{let{title:n="",description:t="",pathname:l="",image:o="",children:s=null,canonicalUrl:c=""}=e;const m=(0,r.A)(),{siteTitle:d,siteTitleAlt:g,siteUrl:p,siteDescription:u,siteImage:h,author:f,siteLanguage:b}=m,y={title:n?`${n} | ${d}`:g,description:t||u,url:`${p}${l||""}`,image:`${p}${o||h}`};return a.createElement(a.Fragment,null,a.createElement("html",{lang:b}),a.createElement("title",null,y.title),a.createElement("meta",{name:"description",content:y.description}),a.createElement("meta",{name:"image",content:y.image}),a.createElement("meta",{property:"og:title",content:y.title}),a.createElement("meta",{property:"og:url",content:y.url}),a.createElement("meta",{property:"og:description",content:y.description}),a.createElement("meta",{property:"og:image",content:y.image}),a.createElement("meta",{property:"og:type",content:"website"}),a.createElement("meta",{property:"og:image:alt",content:y.description}),a.createElement("meta",{name:"twitter:card",content:"summary_large_image"}),a.createElement("meta",{name:"twitter:title",content:y.title}),a.createElement("meta",{name:"twitter:url",content:y.url}),a.createElement("meta",{name:"twitter:description",content:y.description}),a.createElement("meta",{name:"twitter:image",content:y.image}),a.createElement("meta",{name:"twitter:image:alt",content:y.description}),a.createElement("meta",{name:"twitter:creator",content:f}),a.createElement("meta",{name:"gatsby-theme",content:"@lekoarts/gatsby-theme-minimal-blog"}),a.createElement("link",{rel:"icon",type:"image/png",sizes:"32x32",href:(0,i.withPrefix)("/favicon-32x32.png")}),a.createElement("link",{rel:"icon",type:"image/png",sizes:"16x16",href:(0,i.withPrefix)("/favicon-16x16.png")}),a.createElement("link",{rel:"apple-touch-icon",sizes:"180x180",href:(0,i.withPrefix)("/apple-touch-icon.png")}),c?a.createElement("link",{rel:"canonical",href:c}):null,s)}},9962:function(e,n,t){t.r(n),t.d(n,{Head:function(){return o.p},default:function(){return s}});var a=t(6540),i=t(8453);function r(e){const n=Object.assign({p:"p",span:"span",ul:"ul",li:"li",a:"a",em:"em",h3:"h3",h4:"h4"},(0,i.RP)(),e.components);return a.createElement(a.Fragment,null,a.createElement(n.p,null,"Universe is a ‘hard disk’ full of stimuli. Human brain, transforms these stimuli into data, and data into information, through the cognitive functions. Through\nthe information we receive, we start the hunt of acquiring more and more knowledge."),"\n",a.createElement(n.p,null,"Ackoff (1989) in his article From Data to Wisdom proposed a model (wisdom hierarchy) including the following levels:\nData --\x3e Information --\x3e Knowledge --\x3e Insight --\x3e Wisdom\nHis model could be considered as pyramid as each level includes the previous levels."),"\n",a.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; "\n    >\n      <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 31.25%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAABW0lEQVR42iWRy5KbMBBF+f2sss0qVVknP5BUZWaCgUEY85ARSOJloMzEqfzDSQ9Z6bYet28fBY/Hg6ZpKMuSbdsYxxGlFGF4omtbbNcddZIkeO8Zh0F0TNtalmVnGDxxnKCvnrfffwn2fT9MejlYlgUrjzrnubYGPw5cO4NuDHXb4Hp/7JXaiK7YbiFuWCmqgnn8zp89J5jmCT/0OO8Y5PJF1xjRlTF03hJlKS+niCzPKY3mNc8Io5S8CCXtT1T+A6M+YOtP7Osrwbqth9G8zFjXibHHDI5u6qkk3aWseIpPxMWZqq5Rl4LnJCLMFa17w9SfybIvPJ8t47wR3JabxHes63YY1rqhsZbbushoDUmaEcUxiUoJ0zNWfxWGL/wSpiqXUc1HCvWNp5NwFObB/X5nmmdh2B9JnXO8N7GyvvM0pj14VlrLJ0mKXtOY/7WzjmmoacSolPS3eeEf2h6hJC8KDOUAAAAASUVORK5CYII=\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="journey"\n        title=""\n        src="/static/e58d852995b1d1b5b5f4feea053e9506/7d769/journey.png"\n        srcset="/static/e58d852995b1d1b5b5f4feea053e9506/5243c/journey.png 240w,\n/static/e58d852995b1d1b5b5f4feea053e9506/ab158/journey.png 480w,\n/static/e58d852995b1d1b5b5f4feea053e9506/7d769/journey.png 960w,\n/static/e58d852995b1d1b5b5f4feea053e9506/e8a46/journey.png 1282w"\n        sizes="(max-width: 960px) 100vw, 960px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n    </span>'}}),"\n",a.createElement(n.p,null,"Analogously, through software systems, we have created means to capture data, the persuit remains to gain knowledge, insight and wisdom\nin an efficient way and cost-effective way. Efficiency implies"),"\n",a.createElement(n.ul,null,"\n",a.createElement(n.li,null,"Time taken to arrive at an actionable insight"),"\n"),"\n",a.createElement(n.p,null,"With LLM, we have language based world model which can predict output given a natural language ask. Through it training over large internet level data,\nit has learned the latent rules which underlie human language"),"\n",a.createElement(n.p,null,a.createElement(n.a,{href:"https://ai.mukeshkr.me/reasoners/"},"Reasoning model")," take it a step forward by providing an ability to reason and plan for an ask at inference time. They demonstrate generating very long reasoning\nprocess and conduct human-like reasoning actions like clarifying and decomposing questions, reflecting and correcting previous mistakes, exploring new solutions when encountering failure modes."),"\n",a.createElement(n.p,null,"These two technologies together provide an means to add ",a.createElement(n.em,null,"agency")," (autonomous) to the journey from data to insight, which almost seems magical."),"\n",a.createElement(n.p,null,"Performance of reasoning models consistently improves with increasing the computation of reinforcement learning and\ninference. This denotes 2 paradigm shifts"),"\n",a.createElement(n.ul,null,"\n",a.createElement(n.li,null,"From (self-)supervised learning toward reinforcement learning"),"\n",a.createElement(n.li,null,"From scaling solely training computation to scaling both training and inference computation."),"\n"),"\n",a.createElement(n.h3,null,"Background"),"\n",a.createElement(n.p,null,"Let us introduce some background of reinforcement learning and its connection to LLM in this section. Unlike other\nlearning paradigms, reinforcement learning learns through interaction with the environment, rather\nthan learning from a static training dataset. In reinforcement learning, an agent learns by receiving\nrewards from the environment as it explores. The below figure illustrates the interaction between agent and\nenvironment in reinforcement learning for LLM."),"\n",a.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; "\n    >\n      <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 31.25%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAABC0lEQVR42pWQ3WrDMAyF8/6vslIYu9kvbDd9gfai0JQGsga7iZ04blwncXImubSwqzHDMcL+JB0pwX/OPP+JJHyFEGCtRd00KFYriJdnNHmOc9si9D2maYpwT3HktEbx9Qnx9gpbVTgbE7l7wZYSWaa1yD/ekS8eILZbZIcDSiHQDwMGEseuMdBliezpEd/LBSpqnKV7iOMRgRonfAkprwWp0/xrwhkVOXCXCwy5L08StZIwWmEI053jCaQ8wfv+6rDrOiiloGkUmaYoNmuURQHnHFpqwu7iJLeizO12xG1QkRnm2AwXTm5OOIl31JFTSwmeoHEcEUj8z+IEfvPex73ZuoYn9/zG4h3+AK4azKfDlm2rAAAAAElFTkSuQmCC\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="llm as policy"\n        title=""\n        src="/static/04209d72c97386541abf80c138d16777/7d769/llm_as_policy.png"\n        srcset="/static/04209d72c97386541abf80c138d16777/5243c/llm_as_policy.png 240w,\n/static/04209d72c97386541abf80c138d16777/ab158/llm_as_policy.png 480w,\n/static/04209d72c97386541abf80c138d16777/7d769/llm_as_policy.png 960w,\n/static/04209d72c97386541abf80c138d16777/87339/llm_as_policy.png 1440w,\n/static/04209d72c97386541abf80c138d16777/88b03/llm_as_policy.png 1920w,\n/static/04209d72c97386541abf80c138d16777/108d3/llm_as_policy.png 2202w"\n        sizes="(max-width: 960px) 100vw, 960px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n    </span>'}}),"\n",a.createElement("h4",null,"The visualization of the interaction between agent and environment in reinforcement\nlearning for LLMs. Left: traditional reinforcement learning. Right: reinforcement learning for LLMs.\nThe figure only visualizes the step-level action for simplicity. In fact, the action of LLM can be either\ntoken-, step-, or solution-level. "),"\n",a.createElement(n.h4,null,"Agent"),"\n",a.createElement(n.p,null,"Agent is the entity that interacts with environment, which makes decision according to its\npolicy. Formally, a policy π is a mapping from states to actions. It is often represented as a probability\ndistribution (π(a|s)) over actions given a state s, where the agent selects actions based on these\nprobabilities."),"\n",a.createElement(n.p,null,"In the context of LLMs, an agent refers to LLM itself, its policy specify the probability distribution\nof either token-, step-, or solution-level actions based on the current state. The state s",a.createElement("sub",null,"t")," consists of the\ninput provided to the model at time t, including both user inputs and the model’s earlier outputs. The\naction taken by the model can vary depending on the problem setting; it involves generating a single\ntoken, completing a step, or providing a solution."),"\n",a.createElement(n.h4,null,"Environment"),"\n",a.createElement(n.p,null,"Environment refers to the system or world outside the agent. It responds to agent’s\nactions and provide feedback in terms of next state s",a.createElement("sub",null,"t+1")," and rewards r(s",a.createElement("sub",null,"t"),", a",a.createElement("sub",null,"t"),")"),"\n",a.createElement(n.p,null,"Environmental feedback can be categorized as either deterministic or stochastic. Stochastic feedback\nis characterized by a transition distribution p(s",a.createElement("sub",null,"t+1"),", r",a.createElement("sub",null,"t+1"),"|s",a.createElement("sub",null,"t"),", a",a.createElement("sub",null,"t"),"), as seen in systems like dialogue\nmodels, where user responses are inherently unpredictable. On the other hand, deterministic feedback\ninvolves no randomness, yielding a fixed next state s",a.createElement("sub",null,"t+1")," and reward r(s",a.createElement("sub",null,"t"),", a",a.createElement("sub",null,"t"),"). For instance, when a\nLLM solves a mathematical problem, the transition is deterministic, where the current state s",a.createElement("sub",null,"t")," and\naction at are combined to produce the next state s",a.createElement("sub",null,"t+1"),"."),"\n",a.createElement(n.h3,null,"LLM as a policy"),"\n",a.createElement(n.h4,null,"Policy Initialization"),"\n",a.createElement(n.p,null,"Training an LLM from scratch using reinforcement learning is exceptionally\nchallenging due to its vast action space. Fortunately, we can leverage extensive internet data to pre-\ntrain a language model, establishing a potent initial policy model capable of generating fluent language\noutputs. Moreover, prompt engineering and supervised fine-tuning help models acquire human-like\nreasoning behaviors, enabling them to think systematically and validate their own results. These\napproaches enable models to thoroughly explore their solution spaces, leading to more comprehensive\nproblem-solving capabilities."),"\n",a.createElement(n.h4,null,"Reward Design"),"\n",a.createElement(n.p,null,"Both search and learning require guidance from reward signals to improve the\npolicy. There are different levels of action granularity, each corresponding to varying levels of reward\nsignals granularity, which can be explored further. Additionally, these signals are often sparse or even\nnonexistent in many environments. To transform sparse outcome reward to dense process reward,\nthere are some reward shaping methods ",a.createElement(n.a,{href:"https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf"},"Ng et al., 1999"),". For the environment where the reward\nsignal is unavailable, like the task of story writing, we can learn a reward model from preference data\n",a.createElement(n.a,{href:"https://doi.org/10.48550/arXiv.2204.05862"},"Bai et al., 2022a")," or expert data ",a.createElement(n.a,{href:"https://ai.stanford.edu/~ang/papers/icml00-irl.pdf"},"Ng & Russell, 2000"),".\nThe construction of reward model can further evolve into building a world model ",a.createElement(n.a,{href:"https://doi.org/10.48550/arXiv.2306.02572"},"Dawid & LeCun, 2023"),"."),"\n",a.createElement(n.h4,null,"Search"),"\n",a.createElement(n.p,null,"Search plays a crucial role during both the training and testing phases. The training\ntime search refers to generating training data from search process. The advantage of using search\nto generate training data, as opposed to simple sampling, is that search yields better actions or\nsolutions—i.e., higher-quality training data—thereby enhancing learning effectiveness. During\ninference, search continues to play a vital role in improving the model’s sub-optimal policies. For\ninstance, AlphaGo ",a.createElement(n.a,{href:"https://openreview.net/forum?id=C4OpREezgj"},"Wan et al., 2024")," employs Monte Carlo Tree Search (MCTS) during testing\nto enhance its performance. However, scaling test-time search may lead to inverse scaling due to\ndistribution shift: the policy, reward, and value models are trained on one distribution but evaluated\non a different one ",a.createElement(n.a,{href:"https://proceedings.mlr.press/v202/gao23h.html"},"Gao et al., 2023"),"."),"\n",a.createElement(n.h4,null,"Learning"),"\n",a.createElement(n.p,null,"Learning from human-expert data requires costly data annotation. In contrast, reinforce-\nment learning learns through interactions with the environment, eliminating the need for expensive\ndata annotation and offering the potential for superhuman performance. In this roadmap, reinforcement learning utilizes\ndata generated by search for learning via policy gradient or behavior cloning.\nPolicy gradient methods have high data utilization, as they leverage both positive and negative\nsolutions, whereas behavior cloning is advantageous in terms of simplicity and memory efficiency. A\nprominent example of the iterative interaction between search and learning is AlphaGo Zero ",a.createElement(n.a,{href:"https://doi.org/10.1038/nature24270"},"(Silver\net al., 2017"),", which combines Monte Carlo Tree Search (MCTS) (Metropolis & Ulam, 1949) as the\nsearch algorithm with behavior cloning as the learning method, ultimately achieving superhuman\nperformance in the game of Go."),"\n",a.createElement(n.h3,null,"References"),"\n",a.createElement(n.p,null,"[1] Ackoff, R. L. (1989). From data to wisdom. Journal of applied systems analysis, 16(1), 3-9."))}var l=function(e){void 0===e&&(e={});const{wrapper:n}=Object.assign({},(0,i.RP)(),e.components);return n?a.createElement(n,e,a.createElement(r,e)):r(e)},o=t(1173);function s(e){return a.createElement(o.A,e,a.createElement(l,e))}o.A}}]);
//# sourceMappingURL=component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-post-query-tsx-content-file-path-content-posts-data-to-insight-ramp-index-mdx-e00b64c60f29ea895183.js.map