"use strict";(self.webpackChunkMukesh_K_s_The_Art_of_AI=self.webpackChunkMukesh_K_s_The_Art_of_AI||[]).push([[424],{1173:function(e,n,t){t.d(n,{p:function(){return d},A:function(){return p}});var a=t(6540),i=t(557),r=t(6835),o=t(3328),l=t(7715),s=t(7169);var c=e=>{let{post:n}=e;return null};const m=["16px","8px","4px"].map((e=>`rgba(0, 0, 0, 0.1) 0px ${e} ${e} 0px`));var g=e=>{let{data:{post:n},children:t}=e;return(0,i.Y)(o.A,null,(0,i.Y)(r.DZ,{as:"h1",variant:"styles.h1"},n.title),(0,i.Y)("p",{sx:{color:"secondary",mt:3,a:{color:"secondary"},fontSize:[1,1,2]}},(0,i.Y)("time",null,n.date),n.tags&&(0,i.Y)(a.Fragment,null," — ",(0,i.Y)(l.A,{tags:n.tags})),n.timeToRead&&" — ",n.timeToRead&&(0,i.Y)("span",null,n.timeToRead," min read")),(0,i.Y)("section",{sx:{my:5,".gatsby-resp-image-wrapper":{my:[4,4,5],borderRadius:"4px",boxShadow:m.join(", "),".gatsby-resp-image-image":{borderRadius:"4px"}},variant:"layout.content"}},t),(0,i.Y)(c,{post:n}))};const d=e=>{var n,t,a;let{data:{post:r}}=e;return(0,i.Y)(s.A,{title:r.title,description:r.description?r.description:r.excerpt,image:r.banner?null===(n=r.banner)||void 0===n||null===(t=n.childImageSharp)||void 0===t||null===(a=t.resize)||void 0===a?void 0:a.src:void 0,pathname:r.slug,canonicalUrl:r.canonicalUrl})};function p(e){let{...n}=e;return a.createElement(g,n)}},7715:function(e,n,t){var a=t(557),i=t(6540),r=t(4794),o=t(3601),l=t(2174);n.A=e=>{let{tags:n}=e;const{tagsPath:t,basePath:s}=(0,o.A)();return(0,a.Y)(i.Fragment,null,n.map(((e,n)=>(0,a.Y)(i.Fragment,{key:e.slug},!!n&&", ",(0,a.Y)(r.Link,{sx:e=>{var n;return{...null===(n=e.styles)||void 0===n?void 0:n.a}},to:(0,l.A)(`/${s}/${t}/${e.slug}`)},e.name)))))}},7169:function(e,n,t){var a=t(6540),i=t(4794),r=t(7533);n.A=e=>{let{title:n="",description:t="",pathname:o="",image:l="",children:s=null,canonicalUrl:c=""}=e;const m=(0,r.A)(),{siteTitle:g,siteTitleAlt:d,siteUrl:p,siteDescription:h,siteImage:u,author:f,siteLanguage:v}=m,y={title:n?`${n} | ${g}`:d,description:t||h,url:`${p}${o||""}`,image:`${p}${l||u}`};return a.createElement(a.Fragment,null,a.createElement("html",{lang:v}),a.createElement("title",null,y.title),a.createElement("meta",{name:"description",content:y.description}),a.createElement("meta",{name:"image",content:y.image}),a.createElement("meta",{property:"og:title",content:y.title}),a.createElement("meta",{property:"og:url",content:y.url}),a.createElement("meta",{property:"og:description",content:y.description}),a.createElement("meta",{property:"og:image",content:y.image}),a.createElement("meta",{property:"og:type",content:"website"}),a.createElement("meta",{property:"og:image:alt",content:y.description}),a.createElement("meta",{name:"twitter:card",content:"summary_large_image"}),a.createElement("meta",{name:"twitter:title",content:y.title}),a.createElement("meta",{name:"twitter:url",content:y.url}),a.createElement("meta",{name:"twitter:description",content:y.description}),a.createElement("meta",{name:"twitter:image",content:y.image}),a.createElement("meta",{name:"twitter:image:alt",content:y.description}),a.createElement("meta",{name:"twitter:creator",content:f}),a.createElement("meta",{name:"gatsby-theme",content:"@lekoarts/gatsby-theme-minimal-blog"}),a.createElement("link",{rel:"icon",type:"image/png",sizes:"32x32",href:(0,i.withPrefix)("/favicon-32x32.png")}),a.createElement("link",{rel:"icon",type:"image/png",sizes:"16x16",href:(0,i.withPrefix)("/favicon-16x16.png")}),a.createElement("link",{rel:"apple-touch-icon",sizes:"180x180",href:(0,i.withPrefix)("/apple-touch-icon.png")}),c?a.createElement("link",{rel:"canonical",href:c}):null,s)}},8847:function(e,n,t){t.r(n),t.d(n,{Head:function(){return l.p},default:function(){return s}});var a=t(6540),i=t(8453);function r(e){const n=Object.assign({p:"p",ul:"ul",li:"li",h3:"h3",span:"span",h4:"h4"},(0,i.RP)(),e.components);return a.createElement(a.Fragment,null,a.createElement(n.p,null,"The field of Artificial Intelligence (AI) has witnessed unprecedented exploration and advancement of\nLarge Language Models (LLMs) over the past two years. LLMs have progressively evolved to handle\nincreasingly sophisticated tasks such as programming and solving advanced mathematical problems. Reasoning models\nlike OpenAI's o-series model and Deepseek's R1 models, demonstrates generating very long reasoning process\nand conduct human-like reasoning actions like clarifying and decomposing questions, reflecting and\ncorrecting previous mistakes, exploring new solutions when encountering failure modes."),"\n",a.createElement(n.p,null,"Performance of reasoning models consistently improves with increasing the computation of reinforcement learning and\ninference. This denotes 2 paradigm shifts"),"\n",a.createElement(n.ul,null,"\n",a.createElement(n.li,null,"From (self-)supervised learning toward reinforcement learning"),"\n",a.createElement(n.li,null,"From scaling solely training computation to scaling both training and inference computation."),"\n"),"\n",a.createElement(n.h3,null,"Background"),"\n",a.createElement(n.p,null,"Let us introduce some background of reinforcement learning and its connection to LLM in this section. Unlike other\nlearning paradigms, reinforcement learning learns through interaction with the environment, rather\nthan learning from a static training dataset. In reinforcement learning, an agent learns by receiving\nrewards from the environment as it explores. The below figure illustrates the interaction between agent and\nenvironment in reinforcement learning for LLM."),"\n",a.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; "\n    >\n      <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 31.25%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAABC0lEQVR42pWQ3WrDMAyF8/6vslIYu9kvbDd9gfai0JQGsga7iZ04blwncXImubSwqzHDMcL+JB0pwX/OPP+JJHyFEGCtRd00KFYriJdnNHmOc9si9D2maYpwT3HktEbx9Qnx9gpbVTgbE7l7wZYSWaa1yD/ekS8eILZbZIcDSiHQDwMGEseuMdBliezpEd/LBSpqnKV7iOMRgRonfAkprwWp0/xrwhkVOXCXCwy5L08StZIwWmEI053jCaQ8wfv+6rDrOiiloGkUmaYoNmuURQHnHFpqwu7iJLeizO12xG1QkRnm2AwXTm5OOIl31JFTSwmeoHEcEUj8z+IEfvPex73ZuoYn9/zG4h3+AK4azKfDlm2rAAAAAElFTkSuQmCC\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="llm as policy"\n        title=""\n        src="/static/04209d72c97386541abf80c138d16777/7d769/llm_as_policy.png"\n        srcset="/static/04209d72c97386541abf80c138d16777/5243c/llm_as_policy.png 240w,\n/static/04209d72c97386541abf80c138d16777/ab158/llm_as_policy.png 480w,\n/static/04209d72c97386541abf80c138d16777/7d769/llm_as_policy.png 960w,\n/static/04209d72c97386541abf80c138d16777/87339/llm_as_policy.png 1440w,\n/static/04209d72c97386541abf80c138d16777/88b03/llm_as_policy.png 1920w,\n/static/04209d72c97386541abf80c138d16777/108d3/llm_as_policy.png 2202w"\n        sizes="(max-width: 960px) 100vw, 960px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n    </span>'}}),"\n",a.createElement(n.p,null,"The visualization of the interaction between agent and environment in reinforcement\nlearning for LLMs. Left: traditional reinforcement learning. Right: reinforcement learning for LLMs.\nThe figure only visualizes the step-level action for simplicity. In fact, the action of LLM can be either\ntoken-, step-, or solution-level."),"\n",a.createElement(n.h4,null,"Agent"),"\n",a.createElement(n.p,null,"Agent is the entity that interacts with environment, which makes decision according to its\npolicy. Formally, a policy π is a mapping from states to actions. It is often represented as a probability\ndistribution (π(a|s)) over actions given a state s, where the agent selects actions based on these\nprobabilities."),"\n",a.createElement(n.p,null,"In the context of LLMs, an agent refers to LLM itself, its policy specify the probability distribution\nof either token-, step-, or solution-level actions based on the current state. The state st consists of the\ninput provided to the model at time t, including both user inputs and the model’s earlier outputs. The\naction taken by the model can vary depending on the problem setting; it involves generating a single\ntoken, completing a step, or providing a solution."),"\n",a.createElement(n.h4,null,"Environment"),"\n",a.createElement(n.p,null,"Environment refers to the system or world outside the agent. It responds to agent’s\nactions and provide feedback in terms of next state st+1 and rewards r(st, at)"),"\n",a.createElement(n.p,null,"Environmental feedback can be categorized as either deterministic or stochastic. Stochastic feedback\nis characterized by a transition distribution p(st+1, rt+1|st, at), as seen in systems like dialogue\nmodels, where user responses are inherently unpredictable. On the other hand, deterministic feedback\ninvolves no randomness, yielding a fixed next state st+1 and reward r(st, at). For instance, when a\nLLM solves a mathematical problem, the transition is deterministic, where the current state st and\naction at are combined to produce the next state st+1."),"\n",a.createElement(n.h3,null,"LLM as a policy agent"),"\n",a.createElement(n.h4,null,"Policy Initialization"),"\n",a.createElement(n.p,null,"Training an LLM from scratch using reinforcement learning is exceptionally\nchallenging due to its vast action space. Fortunately, we can leverage extensive internet data to pre-\ntrain a language model, establishing a potent initial policy model capable of generating fluent language\noutputs. Moreover, prompt engineering and supervised fine-tuning help models acquire human-like\nreasoning behaviors, enabling them to think systematically and validate their own results. These\napproaches enable models to thoroughly explore their solution spaces, leading to more comprehensive\nproblem-solving capabilities."),"\n",a.createElement(n.h4,null,"Reward"),"\n",a.createElement(n.p,null,"Design Both search and learning require guidance from reward signals to improve the\npolicy. There are different levels of action granularity, each corresponding to varying levels of reward\nsignals granularity, which can be explored further. Additionally, these signals are often sparse or even\nnonexistent in many environments. To transform sparse outcome reward to dense process reward,\nthere are some reward shaping methods (Ng et al., 1999). For the environment where the reward\nsignal is unavailable, like the task of story writing, we can learn a reward model from preference data\n(Bai et al., 2022a) or expert data (Ng & Russell, 2000).The construction of reward model can further\nevolve into building a world model (Dawid & LeCun, 2023)."),"\n",a.createElement(n.h4,null,"Search"),"\n",a.createElement(n.p,null,"Search plays a crucial role during both the training and testing phases. The training\ntime search refers to generating training data from search process. The advantage of using search\nto generate training data, as opposed to simple sampling, is that search yields better actions or\nsolutions—i.e., higher-quality training data—thereby enhancing learning effectiveness. During\ninference, search continues to play a vital role in improving the model’s sub-optimal policies. For\ninstance, AlphaGo (Wan et al., 2024) employs Monte Carlo Tree Search (MCTS) during testing\nto enhance its performance. However, scaling test-time search may lead to inverse scaling due to\ndistribution shift: the policy, reward, and value models are trained on one distribution but evaluated\non a different one (Gao et al., 2023)."),"\n",a.createElement(n.h4,null,"Learning"),"\n",a.createElement(n.p,null,"Learning from human-expert data requires costly data annotation. In contrast, reinforce-\nment learning learns through interactions with the environment, eliminating the need for expensive\ndata annotation and offering the potential for superhuman performance. In this roadmap, reinforce-\nment learning utilizes data generated by search for learning via policy gradient or behavior cloning.\nPolicy gradient methods have high data utilization, as they leverage both positive and negative\nsolutions, whereas behavior cloning is advantageous in terms of simplicity and memory efficiency. A\nprominent example of the iterative interaction between search and learning is AlphaGo Zero (Silver\net al., 2017), which combines Monte Carlo Tree Search (MCTS) (Metropolis & Ulam, 1949) as the\nsearch algorithm with behavior cloning as the learning method, ultimately achieving superhuman\nperformance in the game of Go."))}var o=function(e){void 0===e&&(e={});const{wrapper:n}=Object.assign({},(0,i.RP)(),e.components);return n?a.createElement(n,e,a.createElement(r,e)):r(e)},l=t(1173);function s(e){return a.createElement(l.A,e,a.createElement(o,e))}l.A}}]);
//# sourceMappingURL=component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-post-query-tsx-content-file-path-content-posts-llm-as-a-reasoning-agent-index-mdx-ed1c168a901645908067.js.map