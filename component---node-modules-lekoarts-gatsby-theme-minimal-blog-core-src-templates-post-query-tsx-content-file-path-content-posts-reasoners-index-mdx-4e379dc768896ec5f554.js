"use strict";(self.webpackChunkMukesh_K_s_The_Art_of_AI=self.webpackChunkMukesh_K_s_The_Art_of_AI||[]).push([[439],{1173:function(e,n,t){t.d(n,{p:function(){return g},A:function(){return u}});var a=t(6540),i=t(557),r=t(6835),l=t(3328),o=t(7715),s=t(7169);var c=e=>{let{post:n}=e;return null};const m=["16px","8px","4px"].map((e=>`rgba(0, 0, 0, 0.1) 0px ${e} ${e} 0px`));var p=e=>{let{data:{post:n},children:t}=e;return(0,i.Y)(l.A,null,(0,i.Y)(r.DZ,{as:"h1",variant:"styles.h1"},n.title),(0,i.Y)("p",{sx:{color:"secondary",mt:3,a:{color:"secondary"},fontSize:[1,1,2]}},(0,i.Y)("time",null,n.date),n.tags&&(0,i.Y)(a.Fragment,null," — ",(0,i.Y)(o.A,{tags:n.tags})),n.timeToRead&&" — ",n.timeToRead&&(0,i.Y)("span",null,n.timeToRead," min read")),(0,i.Y)("section",{sx:{my:5,".gatsby-resp-image-wrapper":{my:[4,4,5],borderRadius:"4px",boxShadow:m.join(", "),".gatsby-resp-image-image":{borderRadius:"4px"}},variant:"layout.content"}},t),(0,i.Y)(c,{post:n}))};const g=e=>{var n,t,a;let{data:{post:r}}=e;return(0,i.Y)(s.A,{title:r.title,description:r.description?r.description:r.excerpt,image:r.banner?null===(n=r.banner)||void 0===n||null===(t=n.childImageSharp)||void 0===t||null===(a=t.resize)||void 0===a?void 0:a.src:void 0,pathname:r.slug,canonicalUrl:r.canonicalUrl})};function u(e){let{...n}=e;return a.createElement(p,n)}},7715:function(e,n,t){var a=t(557),i=t(6540),r=t(4794),l=t(3601),o=t(2174);n.A=e=>{let{tags:n}=e;const{tagsPath:t,basePath:s}=(0,l.A)();return(0,a.Y)(i.Fragment,null,n.map(((e,n)=>(0,a.Y)(i.Fragment,{key:e.slug},!!n&&", ",(0,a.Y)(r.Link,{sx:e=>{var n;return{...null===(n=e.styles)||void 0===n?void 0:n.a}},to:(0,o.A)(`/${s}/${t}/${e.slug}`)},e.name)))))}},7169:function(e,n,t){var a=t(6540),i=t(4794),r=t(7533);n.A=e=>{let{title:n="",description:t="",pathname:l="",image:o="",children:s=null,canonicalUrl:c=""}=e;const m=(0,r.A)(),{siteTitle:p,siteTitleAlt:g,siteUrl:u,siteDescription:d,siteImage:h,author:f,siteLanguage:b}=m,v={title:n?`${n} | ${p}`:g,description:t||d,url:`${u}${l||""}`,image:`${u}${o||h}`};return a.createElement(a.Fragment,null,a.createElement("html",{lang:b}),a.createElement("title",null,v.title),a.createElement("meta",{name:"description",content:v.description}),a.createElement("meta",{name:"image",content:v.image}),a.createElement("meta",{property:"og:title",content:v.title}),a.createElement("meta",{property:"og:url",content:v.url}),a.createElement("meta",{property:"og:description",content:v.description}),a.createElement("meta",{property:"og:image",content:v.image}),a.createElement("meta",{property:"og:type",content:"website"}),a.createElement("meta",{property:"og:image:alt",content:v.description}),a.createElement("meta",{name:"twitter:card",content:"summary_large_image"}),a.createElement("meta",{name:"twitter:title",content:v.title}),a.createElement("meta",{name:"twitter:url",content:v.url}),a.createElement("meta",{name:"twitter:description",content:v.description}),a.createElement("meta",{name:"twitter:image",content:v.image}),a.createElement("meta",{name:"twitter:image:alt",content:v.description}),a.createElement("meta",{name:"twitter:creator",content:f}),a.createElement("meta",{name:"gatsby-theme",content:"@lekoarts/gatsby-theme-minimal-blog"}),a.createElement("link",{rel:"icon",type:"image/png",sizes:"32x32",href:(0,i.withPrefix)("/favicon-32x32.png")}),a.createElement("link",{rel:"icon",type:"image/png",sizes:"16x16",href:(0,i.withPrefix)("/favicon-16x16.png")}),a.createElement("link",{rel:"apple-touch-icon",sizes:"180x180",href:(0,i.withPrefix)("/apple-touch-icon.png")}),c?a.createElement("link",{rel:"canonical",href:c}):null,s)}},7821:function(e,n,t){t.r(n),t.d(n,{Head:function(){return o.p},default:function(){return s}});var a=t(6540),i=t(8453);function r(e){const n=Object.assign({p:"p",span:"span",h4:"h4",a:"a",h3:"h3",ul:"ul",li:"li"},(0,i.RP)(),e.components);return a.createElement(a.Fragment,null,a.createElement(n.p,null,"Reasoning models like OpenAI's o1 to o3 and Deepseek's zero to R1  represents a significant milestone in Artificial\nIntelligence which achieves expert-level performances on many challenging tasks that require strong reasoning ability.\nThey can generate very long reasoning process and conduct human-like reasoning actions like clarifying and decomposing\nquestions, reflecting and correcting previous mistakes, exploring new solutions when encountering failure modes."),"\n",a.createElement(n.p,null,"While instruction-tuned models demonstrate general task competency and user intent understanding,\nreasoning models require a more sophisticated repertoire of human-like reasoning capabilities to fully\nleverage their potential."),"\n",a.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; "\n    >\n      <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 50.83333333333333%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAABZUlEQVR42m1Sa2/CMAzk//+oPfg2EGPTeI4NKKUNg0ILTdOWPG5OuiJgtXSJ72RbziktUBhj/qDvYBq4aeAGdbTsoZTCWUrwXCIrFbhFISEINndaoSuen5EVVtNIqb4oJRT12hlXA0nQQHd6wHPfx2NvhbHPsdwWeKC83Se8rbGKzhgHBdrvazz1luh8JtjzajtJQ282VNqAHQqsd5nD7nRGkin4W46AeLATSIQiXSKIhKsJqd5u2jjw2ocqjIP16l675VXYV948WSqD19kOL8OQwDBZJWCxQndC2iDA23eMTaLwxXJ0xht0RiE+linirHFDO1Bj6p8w8hIMFjHmLMXPURE/YjDfY7xOsSVufR1SzXAZYxbmOArtNr0ZqLV2gn26zWsLbG5x/T1qrslzqaoe21tb1rq4cdVgI4oieJ4Hxpi7F4uFu+tcCIF6mX//sCnKskSWZcjz3N2cc6Rpesnrf3cfv6HbBaPTWJajAAAAAElFTkSuQmCC\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="human like reasoning behaviors"\n        title=""\n        src="/static/c229f4f62f174dfc4b785563ae0d933b/7d769/human_like_reasoning_behaviors.png"\n        srcset="/static/c229f4f62f174dfc4b785563ae0d933b/5243c/human_like_reasoning_behaviors.png 240w,\n/static/c229f4f62f174dfc4b785563ae0d933b/ab158/human_like_reasoning_behaviors.png 480w,\n/static/c229f4f62f174dfc4b785563ae0d933b/7d769/human_like_reasoning_behaviors.png 960w,\n/static/c229f4f62f174dfc4b785563ae0d933b/87339/human_like_reasoning_behaviors.png 1440w,\n/static/c229f4f62f174dfc4b785563ae0d933b/73d34/human_like_reasoning_behaviors.png 1464w"\n        sizes="(max-width: 960px) 100vw, 960px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n    </span>'}}),"\n",a.createElement(n.h4,null,"Problem Analysis"),"\n",a.createElement(n.p,null,"Problem analysis serves as a crucial initialization process where the model reformulates and analyzes the problem before\nsolving it ",a.createElement(n.a,{href:"https://www.cse.lehigh.edu/~munoz/Publications/GRExplanation2018.pdf"},"(Kondrakunta et al., 2018)"),". This process\ninvolves multiple steps: explicit problem restatement to verify understanding, identification of implicit\nconstraints, and transformation of abstract requirements into concrete, actionable specifications"),"\n",a.createElement(n.h4,null,"Task Decomposition"),"\n",a.createElement(n.p,null,"When encountering complex problems, humans typically decompose them into several manageable subtask. The decomposition process\nis adaptive and context-aware."),"\n",a.createElement(n.h4,null,"Task Completion"),"\n",a.createElement(n.p,null,"Following problem analysis and task decomposition, the model generates solutions through step-by-step reasoning based\non clarified problems and decomposed subtasks. This behavior forms the foundation for all other reasoning processes,\nwhere successful solutions lead to subsequent subtask processing, while problematic solutions trigger\nthe generation of alternatives or self-correction behaviors. Step-by-step generation significantly\nenhances models’ complex reasoning capabilities"),"\n",a.createElement(n.h4,null,"Alternative Proposal"),"\n",a.createElement(n.p,null,"When faced with reasoning obstacles or dead ends, the ability to generate diverse alternative solutions becomes crucial.\nThis systematic exploration of alternatives not only expands the search space but also enables iterative refinement\nthrough solution comparison, leading to more well-reasoned outputs."),"\n",a.createElement(n.h4,null,"Self-Evaluation"),"\n",a.createElement(n.p,null,"Following task completion, self-evaluation serves as a critical verification mechanism to validate the correctness\nof proposed solutions. This evaluation capability can be enhanced through two primary approaches: implementing detailed\nevaluation criteria to instill self-evaluation abilities or utilizing self-debate for cross-validation"),"\n",a.createElement(n.h4,null,"Self-Correction"),"\n",a.createElement(n.p,null,"When encountering manageable errors in the reasoning process, models employ self-correction behaviors to address them."),"\n",a.createElement(n.p,null,"OpenAI has outlined a ",a.createElement(n.a,{href:"/open-a-is-5-stages-to-agi/"},"5-stage roadmap to AGI"),", with stage 2 focusing on becoming a strong reasoner, stage 3 centered on becoming an\nagent. The model like OpenAI's o1 to o3 and Deepseek's R1 has already reached stage 2, achieving reasoning capabilities on par with human\nexperts."),"\n",a.createElement(n.h3,null,"LLM as an reasoning agent - Problem Formulation"),"\n",a.createElement(n.h3,null,"Engineering"),"\n",a.createElement(n.ul,null,"\n",a.createElement(n.li,null,a.createElement(n.a,{href:"https://huggingface.co/blog/open-r1"},"Open-R1: a fully open reproduction of DeepSeek-R1"),"\n",a.createElement(n.ul,null,"\n",a.createElement(n.li,null,a.createElement(n.a,{href:"https://github.com/huggingface/open-r1"},"Github link")),"\n"),"\n"),"\n"),"\n",a.createElement(n.h3,null,"References"),"\n",a.createElement(n.ul,null,"\n",a.createElement(n.li,null,a.createElement(n.a,{href:"https://arxiv.org/pdf/2412.14135v1"},"Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective")),"\n",a.createElement(n.li,null,a.createElement(n.a,{href:"https://arxiv.org/pdf/2408.07199"},"Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents")),"\n",a.createElement(n.li,null,a.createElement(n.a,{href:"https://arxiv.org/pdf/2501.12948v1"},"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning")),"\n"))}var l=function(e){void 0===e&&(e={});const{wrapper:n}=Object.assign({},(0,i.RP)(),e.components);return n?a.createElement(n,e,a.createElement(r,e)):r(e)},o=t(1173);function s(e){return a.createElement(o.A,e,a.createElement(l,e))}o.A}}]);
//# sourceMappingURL=component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-post-query-tsx-content-file-path-content-posts-reasoners-index-mdx-4e379dc768896ec5f554.js.map